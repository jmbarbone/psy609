---
title             : "Replication in Social Psychology"
shorttitle        : "Replication Social Psychology"
author: 
  - name          : "Jordan Mark Barbone"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "jb769904@wcupa.edu"
affiliation:
  - id            : "1"
    institution   : "West Chester University of Pennsylvania University"
authornote: |
  Department of Psychology, 125 West Rosedale Avenue, West Chester University of Pennsylvania, West Chester, PA, 19383.
  Paper prepared for PSY609 Advanced Social Psychology.
abstract: |
  One of the most important issues for those in Psychology, especially Social Psychology, is to reason with the continuing pressures stemming from issues of replication.  More research is showing a lack of reliability in research findings, challenging long-held studies, and exposing concerns with adequately conducted research and possible fraud.  Direct replication is rare in Psychology, not sought after by publishers, and often lack other incentives.  An overview of the “replication crisis” will be presented and two specific areas in social psychology will be presented and discussed in relation to these themes.
  
keywords          : "social psychology, replication, false-positives, collaboration"
wordcount         : "X"
bibliography      : references.bib
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
# output            : papaja::apa6_pdf
output            : papaja::apa6_docx
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
# devtools::install_github("crsh/papaja@devel")
library("papaja")

# Some limitations with outputting to MS Word:
# 
# Always add a header line with running head and page number
#
# If necessary,
# 
# - position author note at the bottom of page 1
# - move figures and tables to the end of the manuscript
# - add colon to level 3-headings
# - in figure captions,
# - add a colon following the figure numbers and italicize (e.g. “Figure 1. This is a caption.”)
# 
# in tables,
# - add horizontal rules above the first and below the last row
# - add midrules
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

In the 17th century Robert Boyle's reported success in observing anomalous suspension of water with his sophisticated air pump was put to question.  These findings were difficult to replicate due to the instruments cost and complexity.  Another scientist, Christiaan Huygens was able to demonstrate a similiar effect with his own air pump but was met with the same reaction.  Thus a debate over validating claims and differentiating between "thought experiments" and actual experiments was birthed.  The Royal Society was not satisfied until in 1663 when Huygens was invited to England and able to successfully replicate this phenomenon [@shapin1984pump].

What came from this skeptic event was a growing sense of the necessity of replicability and establishing facts that could be observed by others.  Today, to publish in well-respected journals, researchers must describe the methods of their study or experiment well-enough for the reader to be able to successfully implement the same methodologies and assumingly reach similar conclusions.  Yet, these might only be read and criticized with no follow-through of actual testing.  In this way, the review of an article more akin to the thought experiments Boyle discussed than to the effort of replicating the events.  Replicability is easy to stress through the words of the others, such as Braude [@braude2002esp] who tells us that "only experiments whose results can be repeated are considered genuine and reliable" and that this can be used as a "demarcation criterion between science and non-science".  Hype has been generated around a "replication crisis" with reports of null findings and failures to replicatea -- thus asserting a serious threat the the integretity and character of psychological science.

# What is the "replication crisis"?

Sanja Srivastava maintains a blog, "The Hardest Science" on psychology with insights he holds as a professor and researcher in social psychology.  His blog is named in part to retaliate against the insistance that science exists on a continuum from the soft sciences, such as psychology and sociology, to the hard sciences, like physics [@srivastava2009making].  He argues that each field is equal to the other scientifically as they all seek to answer inquries through applying logic and reasoning to evidence.  However, the issues tackled are clearly different.  Psychology tends to focus on understanding complex systems and tries to find patterns and reason to behavior.  In this way, the issues psychology faces are more difficult to answer clearly.  As such, we may want to discontinue the description of psychology as a "soft science" and adopt the name of his blog, "the hardest science".  This will be an important idea to maintain as issues with psychological replication are put forth and when attention is focused on psychology; or specically social psychology here; and while understanding that these problems are not just unique to us but may span across the whole of science [@ioannidis2005why; @sterne2001sifting].

Researchers understand the scientific importance of conducting replications in providing to provide contuing support for a hypothesis.  This is cheifly served by direct replications but other functionalities ought to be noted, such that demonstrating the same effect can be found with virtually the same conditions provides further evidence against random chance and sampling errors.  Conceptual replications, which should logically occur afterwards, would verify the generalizability of a hypothesis with other possible measures, means, or populations.  And in what is hoped to be a smaller issue but one none-the-less, replications can help control for fraud -- better accomplished when completed by those independent from the original research team.  Although these functionalities seem critical to science, the rate of replications in psychology speaks a different story.

One estimate places the rate of replication in 100 psychology journals at 1.07% of publications [@makel2012replications], with rates of reported replication steadily increasing towards 2.5% in the 2010s.  It is easy to consider why this is the case.  Conducting a replication can be a tedious, time consuming and resource draining process.  The replicating researchers may have to go to additional lengths to understand the methodology of the original researchers to ensure a set-up as similiar to the original as possible.  Even if the replication is a success, there is a general consensus that journals favor novelty and positive finding.

Makel and colleagues [-@makel2012replications] also note that the median number of citations for replication articles was estimated at 17 which, unsurprisingly, was lower than the median rate for the original study at 64.5.  But 17 citations for an article is still a decent accomplishment; only 3 of the 100 journals examined had 5 year impact factors above 17.  This suggests that replications articles, although they may not make frequent appearances, are generally well-recieved by researchers and possibly sought out.  Publishing companies may want to focus on providing new research findings but metrics like these may providing a convincing argument that that accepting replication articles might contribute to higher impact factors and attention to journals.  If the journal published the original article, this might almost represent a sort of "double-dipping" of references.

Replication in psychology is a rare yet rewarded occurence.  However, it is worthwhile to really delve into the necessity of replications.  Afterall, calls and requests for greater replication may require large, consuming endeavors.  Psychology has been functioning with a small amount of replications for quite some time, so the question can be posed, "do we really need more replications?" or "is this replication 'crisis' even that severe?".

Pashler and Harris [-@pashler2012replicability] examined three arguments concerning the magnitude of the replication crisis.  The first argument centers around the standard 5% likelihood of acceptance of a false null hypothesis and 80% likelihood of rejecting a false null hypothesis.  These may seem like safe thresholds but the false positive rate of published articles is likely much higher than face expectancy.  We can calculate the probability that a positive result is false finding ($\alpha^1$) by dividing the proportion of false positives ($P_\alpha$) by the sum of the proportion of false positives and the proportion of correct rejections ($P_\beta$); i.e., $\alpha^1 = \frac{P_\alpha}{P_\alpha + P_\beta}$.
  
With an assumption that the prior probability of a true effect is 10%, and given an \alpha level of .05, $P_\alpha = .05 * (1 - 10) = .045$.  Given a power level of .80, we would find that $\alpha^1 = .045 / (.045 * .80) = .36$.  We can then conclude that given the standard statistical assumptions, and a prior probability of 10%, that 36% of findings are likely to be false.  This is a generous assumption that does not take into account other factors that could lead to biased false positives.

In his brazenly titled article, "Why most published research findings are false", Ioannidis [-@ioannidis2005why] provides a more expansive review of the probability of false positive or exaggerated articles.  His formualae include variations for researcher bias and the number of teams involved in a particular field.  Using his methods, Iaonnidis identifies six correllaries to estimated false positive rates: studies with smaller sizes; studies with smaller effect sizes; the greater number of tests and less pre-selection of tests in a study; the greater the design or methodology flexibility; the greater the financial or interest in publishing the finding; and the more teams involved in a particular area.  His last correllary seems at first counterintuitive without explanation.  Ioannidis proposes that the "hotter" the area of research the more pressure researchers have to disseminate their "impressive" findings.  Ioannidis comes to the conclusion that, given all these factors, the majority of research -- across all fields -- are likely to be false or exaggerated reports of effects.

The second argument Pashler and Harris [-@pashler2012replicability] tackle is the thought that despite a lack of _direct_ replication, _conceptual_ relications are more frequent and test not just the validity of the original research but the generalizability.  The authors echo concerns from Ioannidis [-@ioannidis2005why] that published conceptual replications represent the favored "interesting" findings.  If conceptual replications did provide a more rigourous testing of a hypothesis it might reason that the rate of failure here would be higher than it is.  Of the estimate 1.07% of publications; direct replications constituting only 14% of these [@makel2012replications].  The publication rate for failed direct replications (14.6%) is nearly twice that of failed conceptual replications (7.5%).  This could be result of the difficulty of estimating the likelihood that a false finding is due to the original hypothesis being incorrect, that the theory does not generalize to the specific difference, or that the replication was not designed or executed well enough  [@earp2015replication].

Cohen [-@cohen1994earth] provides insight that can be applied n parallel to this the conceptual replication through criticizing thresholds of null hypothesis significance testing (NHST) and outlines a few misconceptions and misuses of statistical analyses.  Often the _null hypothesis_ is interpreted as a definition of $H_0: \mu = 0$ rather than the hypothesis which is to be nulled.  These tests are not just to compare whether or not there exists an effect but that levels of effect ought ot be compared as well.  A quick note Cohen makes is the absurdity of testing rater reliability: that is, testing whichever computeted statistic against the _null hypothesis_ that there exists no reliability between or amongst raters -- even with a small sample these results would look significant.  This fundemental missuse of NHST may then lead researchers to believe that because they have successfully reject a null hypothesis -- rather than nullified a previous hypothesis -- that there theory must be true [@meehl1990summaries].  Greater understanding of null hypothesis testing may aid in the design terpretation of direct and coneptual replications.

Science, we like to think, is self-correcting.  Pashler and Harris [-@pashler2012replicability] challenge this arguement.  They note their quick Google Scholar search for replication failures showed that the median replication attempt delay was 4 years [^replication_delay] with 10% of replications occuring longer than 10 years.  This is could be taken as a good sign that researchers are quick to scrutinize research with their own attempts.  Howeveer, Pasher and Harris contend that this may simply represent the "faddish" nature of psychological research and that research which has failed to replicate may have also failed to maintain the herd's interest.  Older, possibly less contemporarily interesting research, should also be provided the service of scrutiny and rexamination.

[^replication_delay] I attempted a similiar search by looking at the first 15 articles to which I had immediate access.  The median and approximate mean was 6 years -- although 11 of the replication articles were published in the 80s or 90s.

When 1,500 scientists were surveyed on comments and rates of replications, rouhgly half of those in psychology (approximately 54 respondents) reported having failed to reproduce their own work or someon else's work [@baker2016scientists].  Compared to the whole, approximately 55% of respondents failed to replicate their own work and more than 70% failed to replicate anothers, and only 16% of these respondents successfully published a failure to reproduce.  Self-correction, through publication of failed replications, does not seem to be that great of an argument if these results are all lost to the dreaded file drawer -- or, increasingly appropriate, forgotten flash drive.

Replications, although well-recieved by researchers, are a bit of a rarity in the literature.  However, there also appears to be growing interest as the rates of replications are increasing and as groups are becoming interested in taking on this problem.

# Large-scale replication attempts

The task of reproducibility testing can be performed with single instances and thorough examination of specific results as they pertain to the original article.  Possibly a more effective means of garnering more attention is large-scale replications which either test a great many hypotheses are fewer hypotheses multiple times.  With such a great endeavor, researchers are leveraging cooperation acorss multiple reseaerch sites and labs.

## The Open Science Collaboration

The Open Science Collaboration (OSC) of the Center for Open Science (COS) set out on an endeavor to promote the interest in replication studies and set standards for replications [@open2012open].  The OSC selected 100 contemporary studies across three journals in psychology and and reported the results of their replications.  Of these 100, 93 were originally reported with significant findings[^sig_findings], but only 36 of the replication findings reached the same conclusion.  When separting by discipline, the rate of success for cognitive experiences was greater than social (21/42 and 14/55, respectively[^sig_props])  Possibly coincidentally, this is concordant with the rough estimate that 36% of positive findings in research are false [@pashler2012replicability; @ioannidis2005why].  Original studies that reported greater effect sizes and more significant findings were more likely to be replicated than those with smaller effects and less significance.  

[^sig_findings]: Significant effects here being defing by a reported *p* of less than .05.

[^sig_props]: This reported only on the 97 original articles with reported significance and reported on the those with p < .05 in the original direction.

The COS has made clear possible issues with their findings.  They acknowledge that there is no single standard for determining the success of a replication and thus report on several measures that may be taken into account [@open2012open; @open2015estimating].  Their selection of articles was not entirely random either, and they acknowledge that there are inherently greater challenges in replicating some psychological research that may rely on a specific population or dependent on an historical event.  Thee are also some criticisms regarding some changes made in the replications which might invalidate their status as direct replications [@gilbert2016comment].

## The "Many Labs" Project

In a different method, the "Many Labs" project attempted 36 replications of 13 studies, investigating the varaibility of replication findings [@gilbert2016comment].  This method, although more demanding than the OSC's attempts, provides much a much more critical evaluation of studies.  Variations in effect sizes compared against original reports and _p_ values from each replication were taken into consideration.   Of the 13 articles tested, 10 of them showed clear indications of successful replications.  The articles tested were chosen for their simplicity, ability to be complete both in person or online, and the general level of procesdural replicability -- similiar to the criteria from the OSC.  

# Case studies

## Roasting the "Marshmallow test"

Walter Mischel's now famous Stanford Marshmallow studies have generally found correlates between the delay of gratification to better life outcomes [@mischel1989delay; @ayduk2000regulating].  A conceptual replication identified some criticism of the original studies [@watts2018revisiting].  Namely, the original children tested were from a highly selected sample in the Stanford University community.  The studies also failed to account for possible confounds such as mother's education and home environment.  The sample retained for longitudinal studies were also much lower than their original experiment (35-89 and over 600, respectively).  In their replication, Tyler, Duncan, and Quan utilized data from the National Institude of Child Health and Human Development (NICHD) Study of Early Child Care and Youth Development (SECCYD).  Data included information on a delayed gratification test and behavioral outcomes at age 15.  An important note is that the children were all born of mothers who did not have have or complete a college education.  Only this group was examined due to concerns with truncation of gratification delay measures for children born to mothers who completed college and because the population used is more appropriate and of interest to policy-makers of developmental interventions.

In their analysis, Watts and colleages were able to show a replication of achievement at age 15, although smaller than in the original studies.  They were not able to find significant results with delayed gratification and behavioral measures.  The significant results they found were also moderated by variables such as child background, home environment, and early cognitive skills -- to the point of the interaction of delayed gratification being insignificant.  Albeit this is one study that has dampened the relationship of self-control to later life outcomes, it is an important one.  Displayed here is the concern of controlling for variables that may not have been of interested to the original authors.  Although the initial and follow-up findings have show significance and the track record appears sound, revisiting the study itself highlights concerns.  Replications like these are important for understanding how even ubiquitous findings that seem unanimous in the literature may require additional scrutiny.

## Violence in video-games

There exists a contentious forum between researchers in the field of media whether or not violent media -- specifically video games -- contributes to violent acts by individuals.  A meta-analysis [@anderson2001effects] of violent videogames concluded that violent videogames increase aggression, physiological arousal, and aggression-releated thoughts and feelings -- with small but positive lationships.  Yet, years later, another meta-analysis [@ferguson2007good] found that videogames were not linked to aggression after controlling for publication bias.  This second meta-analysis also reported on improvement on visuospatial cognition.  Here, too, there was an issue with publication bias -- but even when adjusting for this, the improvement effect was still significant.

Now, with conflicting meta-analyses, researchers may be stuck _inter canem et lupum_ when reasoning out how to interpret these findings.  There also exists another bias from either side: the first publication was presented by Brad Bushman and Craig Anderson -- two reserachers known for their findings on the influence of violent media on violent behaviors and attitudes.  The second was published by Christopher John Ferguson, a researcher who quite frequently challenges these views.

Along with these claims, this area of social psychology has been subjected to a complicated scandal centering around Brad Bushman and a doctoral student of his, Jodie Whitaker [^boom_timeline]. In April 2013, Whitaker and Bushman published their article, "Boom, Headshot!": Effect of video game play and controller type on firing aim and accuracy online in the journal _Communication Research_ [-@whitacker2012boom].  An investigation into data quality was initiated by a data request due to concerns over appropriateness of statistical analyses.  Four years later, the article was retracted [@whitaker2017retraction] and Jodie Whitaker's PhD was revoked by Ohio State University.  The senior author was dismissed of all allegations and was able to publish a replication of the study [@bushman2018boom].  However, not all effects were replicated: the main findings of controller type were not found to be significant, but others were.  Further, the effects reported here are small, the significance levels are -- in most cases -- just under .05, and the number of participants have been increased.

[^boom_timeline]: A timeline of the events and correspondenced, as well as document of files and reports exchanged, has been curated by Malte Elson here: http://www.malte-elson.com/headshot

# Future directions in social psychology

It is clear that replication is an issue to be resolved.  The COS has considered ways in which to examine the results of replications [@open2012open; @open2015estimating].  In a similar vein, there need to be a further discussion on how to go about replications.  Designing a replication study may not be as easy as exactly reproducing the same methods from the investigative study [@maxwell2015suffering], particularly around sample size and estimated powers, to the point where we may have to be just as critical of replication articles as the original investigative article.

Replicability is also not well addressed in psychology education and training.  We are taught to think about how our study, our experiment is unique and what it offers.  What's our twist on the topic?  How are we differentiating this work from the work of others?  All in attempts to find your work in a reputable academic journal.  The "publish or perish" attitudes that may be conveyed at some instutitions would lead to prioritization of new ideas than the confirmation of new and current ones.  

Begeley and Iannidis [-@begley2015reproducibility] have currated a list of ways in which we can help correctfor the lack of replications and the overall goodness of science.  Among these are emphasizing greater statistical and experimental methodology knowledge; providing open access to data for examination and analytic replication; using more meta-analytic techniques for establishing general findings in areas of research; lobbying journals to solict replication bids; and pushing institutional responsibiliy, possibly to the point of requiring some level of open access or replicability.  A suggested cultural change is to consider, more greatly, the quality of research and judge academics and other researchers on their reproducibily, openness and sharing.

# Concluding remarks

Consider again the "Hardest Science".  Sanjay Srivastava has prepared a semi-satirical class syllabus on psychological and research methods titled "Everything is F\*\*\*\*ed" [@srivastava2018everything].  Here Dr. Srivastava has curated a selection of articles to highlight shortcomings in psychological research that range from uninterpretable results, failures to replicate, and poor statistical analyses.  Yet he still offers hope the everything is okay towards the end of the semester.  Direct replication is an issue in psychology - especially in the social realm, but that that does not mean that all past work is questionable.  As mentioned earlier, *indirect* and *conceptual* replications occur often.  For now, these will have to do to demonstrate that phenomonon of past research is sound and replicable.

We should leave with a few notes of caution and tips.  Research reproduced outside the original laboratory or collaborators aids more in demonstrating reproducibiliy than continuous articles from the same institution.  Although most published replications have at least one of the original authors [@makel2012replications]  Failure to replicate does not mean that the original effect is invalid.  As the COS has suggested, the issue may even be with the attempt the replicate and methodological differences that were overlooked or unnoticed [@open2012open].  There may exists some concepts and theories that are downright wrong and invalid but shotty research continues to push these to publishers.  In the other sense, a good idea may have gone unnoticed because the first experiment was not successful.  But we ought not to disqualify any large portion of psychology simply because we have some concerns about reproducibility.  Early findings have been a little grim, but these have tended towards testing recent research rather than anything foundational.

--

Two schools of though, extreme in opposition, can be considered when reacting to the overall threat of the "replication crisis".  To take the pessimistic meta-inductionism approach would be to begin throwing out research on the grounds that the previous works have been falsified, disproved, or _un-proven_.  It would take that if studies and theories, which were one or still are held as truth are incorrect, why should be not dismiss the bulk of what we know?  The epistemic optimist would asser that through rigourous scientific process, what we know now is true, or the approximately true.  Yes, some long-held concepts will be dismantled and others held in a possibly state of limbo of acceptance, but we cannot simply disregard everything and start at square one again.  

--

Not matter the results, reproducibility, openness, and transparancy will continue to grow from these expeditions.  Already the ability to directly share research is made easy with online storage and sharing.  Journals are requiring data in order to publish -- some even with requirements that data must be publically available [@jhl2018academic].  Often it may seem, when reading the interpreations of researcher's work, that hypotheses are strongly confirmed when the data suggest a weaker magnitude with more exceptions.

\newpage

# References
```{r create_r-references}
r_refs(file = "rreferences.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
