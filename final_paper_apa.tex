\documentclass[man]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Replication in Social Psychology},
            pdfauthor={Jordan Mark Barbone},
            pdfkeywords={Social psychology, Replication, Collaboration},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}


  \title{Replication in Social Psychology}
    \author{Jordan Mark Barbone\textsuperscript{1}}
    \date{}
  
\shorttitle{Replication Social Psychology}
\affiliation{
\vspace{0.5cm}
\textsuperscript{1} West Chester University of Pennsylvania University}
\keywords{Social psychology, Replication, Collaboration\newline\indent Word count: X}
\usepackage{csquotes}
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

\usepackage{longtable}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[flushleft]{threeparttable}
\usepackage{threeparttablex}

\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}


\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers

\authornote{Department of Psychology, 125 West Rosedale
Avenue, West Chester University of Pennsylvania, West Chester, PA,
19383. Paper prepared for PSY609 Advanced Social Psychology.

Correspondence concerning this article should be addressed to Jordan
Mark Barbone, Postal address. E-mail:
\href{mailto:jb769904@wcupa.edu}{\nolinkurl{jb769904@wcupa.edu}}}

\abstract{
One of the most important issues for those in Psychology, especially
Social Psychology, is to reason with the continuing pressures stemming
from issues of replication. More research is showing a lack of
reliability in research findings, challenging long-held studies, and
exposing concerns with adequately conducted research and possible fraud.
Direct replication is rare in Psychology, not sought after by
publishers, and often lack other incentives. An overview of the
``replication crisis'' will be presented and two specific areas in
social psychology will be presented and discussed in relation to these
themes.


}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

In the 17th century Robert Boyle reported success in observing anomalous
suspension of water with his sophisticated air pump was put to question.
These findings were difficult to replicate due to the instruments
complexity and expence. Another scientistChristiaan Huygens was able to
demonstrate a similiar effect with his own air pump but was met with the
same reaction. Thus a debate over validating claims and differentiating
between \enquote{thought experiments} and actual experiments was
birthed. The Royal Society was not satisfied until in 1663 when Huygens
was invited to England and was able to successfully replicate this
phenomenon (Shapin, 1984).

What came from this skeptic event was a growing sense of the necessity
of replicability and establishing facts that could be observed by
others. Today, to publish in well-respected journals, researchers must
describe the methods of their study or experiment well-enough for the
reader to be able to successfully implement the same methodologies to
come to similar conclusions. Yet, these might only be read and
criticized without following through with tests. In this way, the review
of an article more akin to the thought experiments Boyle discussed than
to the effort of replicating the events. Replicability is easy to stress
through the words of the others, such as Braude (Braude, 2002) who tells
us that \enquote{only experiments whose results can be repeated are
considered genuine and reliable} and that this can be used as a
\enquote{demarcation criterion between science and non-science}.

\hypertarget{what-is-the-replication-crisis}{%
\section{\texorpdfstring{What is the \enquote{replication
crisis}?}{What is the ``replication crisis''?}}\label{what-is-the-replication-crisis}}

Sanja Srivastava maintains a blog, \enquote{The Hardest Science} which
on psychology with insites he holds as a professor and researcher in
Social Psychology. His blog is named in part to retaliate against the
insistance that science exists on a continuum from the soft sciences
such as psychology and sociology, to the hard sciences like physics
(Srivastava, 2009). He argues that each field is equal as sciences as
they all seek to answer inquries through applying logic and reasoning to
evidence. However, the issues tackled are clearly different. Psychology
tends to focus on understanding complex systems and tries to find
patterns and reason to behavior. In this way, the issues psychology
faces are more difficult to clearly answer. As such, we may want to
discontinue the description of psychology as a \enquote{soft science}
and adopt \enquote{the hardest science}. This will be an important idea
to maintain as issues with psychological replication are put forth and
when attention is focused on psychology; or specically social psychology
here. These problems are not just unique to these fields but may span
across the whole of science (Ioannidis, 2005; Sterne \& Smith, 2001).

Arguably the easiest identifier of concern is the rate of replication
studies in psychological research. One estimate places the rate of
replication in scientific journals at 1.07\% of publications (Makel,
Plucker, \& Hegarty, 2012). (Cohen, 1962)

Pashler and Harris (2012) examined three arguments concerning the
magnitude of the replication crisis. The first argument centers around
the standard 5\% likelihood of acceptive a false null hypothesis and
80\% likelihood of rejecting a false null hypothesis. These may seem
like safe thresholds but the false positive rate is higher than at first
glance. We can calculate the probability that a positive result is false
(\(\alpha^1\)) by dividing the proportion of false positives
(\(P_\alpha\)) by the sum of the proportion of false positives and the
proportion of correct rejections (\(P_\beta\)); i.e.,
\(\alpha^1 = \frac{P_\alpha}{P_\alpha + P_\beta}\).

With an assumption that the prior probability of a true effect is 10\%,
and given an \alpha level of .05, \(P_\alpha = .05 * (1 - 10) = .045\).
Given a power level of .80, we would find that
\(\alpha^1 = .045 / (.045 *s .80) = .36\). A rate of .36 for false
positives findings, given these criteria, should amplify the concern.

In his brazenly titled article, \enquote{Why most published research
findings are false}, Ioannidis (2005) provides a more expansive review
of the probability of false positive articles. His formualae include
variations for researcher bias and number of teams involved. Using his
methods, Iaonnidis identifies six correllaries to estimated false
positive rates: studies with smaller sizes; studies with smaller effect
sizes; the greater number of tests and less pre-selection of tests in a
study; the greater the design or methodology flexibility; the greater
the financial or interest in publishing the finding; and the more teams
involved in a particular area. His last correllary seems at first
counterintuitive without explanation. Ioannidis proposes that the
\enquote{hotter} the area of research the more pressure researchers have
to disseminate their \enquote{impressive} findings. Ioannidis comes to
the conclusion that, given all these factors, more the majority of
research -- across all fields -- are likely to be false findings.

The second argument Pashler and Harris (2012) tackle is suggestiong that
despite a lack of direct replication, conceptual relications are more
frequent and test not just the validity of the original research but the
generability, too. The authors echo concerns from Ioannidis (2005) that
published conceptual replications represent \enquote{interesting}
findings that publications favor. The rate of published replication in
psychology is estimated at 1.07\% of publications; direct replications
constituting only 14\% of these (Makel et al., 2012). The publication
rate for failed direct replications is nearly twice that of failed
conceptual replications (14.6\% and 7.5\%, respectively).

Cohen ({\textbf{???}}) takes another approach to criticizing thresholds
of null hypothesis significance testing (NHST) and outlines a few
misconceptions and misuses of statistical analyses. Often the \emph{null
hypothesis} is interpreted as a definition of \(H_0: \mu = 0\) rather
than the hypothesis which is to be nulled. These tests are not just to
compare whether or not there exists an effect but that levels of effect
ought ot be compared as well. A quick note Cohen makes is the absurdity
of testing rater reliability: that is, testing whichever computeted
statistic against the \emph{null hypothesis} that there exists no
reliability between or amongst raters -- even with a small sample these
results would look significant. This fundemental missuse of NHST may
then lead researchers to believe that because they have successfully
reject a null hypothesis -- rather than nullified a previous hypothesis
-- that there theory must be true (Meehl, 1990).

\hypertarget{false-positive-psychology}{%
\subsection{False positive-psychology}\label{false-positive-psychology}}

\hypertarget{estimating-the-reproducibility}{%
\subsection{Estimating the
reproducibility\ldots{}}\label{estimating-the-reproducibility}}

The Open Science Collaboration (OSC) of the Center for Open Science
(COS) set out on an endeavor to promote the interest in replication
studies and set standards for replications ({\textbf{???}}). The OSC
selected 100 contemporary studies across three journals in psychology
and and reported the results of their replications. Of these 100, 93
were originally reported with significant findings\footnote{Significant
  effects here being defing by a reported \emph{p} of less than .05.},
but only 36 of the replication findings reached the same conclusion.
When separting by discipline, the rate of success for cognitive
experiences was greater than social (21/42 and 14/55,
respectively\footnote{This reported only on the 97 original articles
  with reported significance and reported on the those with p
  \textless{} .05 in the original direction.}) Possibly coincidentally,
this is concordant with the rough estimate that 36\% of positive
findings in research are false (Ioannidis, 2005; Pashler \& Harris,
2012). Original studies that reported greater effect sizes and more
significant findings were more likely to be replicated than those with
smaller effects and less significance.

The COS has made clear possible issues with their findings. They
acknowledge that there is no single standard for determining the success
of a replication and thus report on several measures that may be taken
into account (Open Science Collaboration, 2012, 2015). Their selection
of articles was not entirely random either, and they acknowledge that
there are inherently greater challenges in replicating some
psychological research that may rely on a specific population or
dependent on an historical event.

\hypertarget{why-replications-dont-occur}{%
\section{Why replications don't
occur}\label{why-replications-dont-occur}}

\hypertarget{why-is-this-even-a-crisis}{%
\subsection{Why is this even a
crisis?}\label{why-is-this-even-a-crisis}}

Publishers tend to value in experiments and studies over replications.
Confirmatory replications may not caught the attention of a journal
because \enquote{\emph{we already know that}}. Face value, repeated
someone else's work does not seem to contribute much to the scientific
field. Yet, replicable \enquote{serves as a kind of demarcation
criterion between science and non-science} (Braude, 2002, p. 33).
Science is science because it is replicable.

\hypertarget{data-quality-and-ethics}{%
\subsection{Data quality and ethics}\label{data-quality-and-ethics}}

No one is possibly more infamous for fabricating data than Andrew
Wakefield. This well known example of a researcher fabricating data
related to autism spectrum distorder and the MRR vaccine. Yet,
incidences like these may be more frequent but more benign.

\hypertarget{case-studies}{%
\section{Case studies}\label{case-studies}}

\hypertarget{roasting-the-marshmallow-test}{%
\subsection{\texorpdfstring{Roasting the \enquote{Marshmallow
test}}{Roasting the ``Marshmallow test''}}\label{roasting-the-marshmallow-test}}

In the mid 1960's, Walter Mischel and

\hypertarget{violence-in-video-games}{%
\subsection{Violence in video-games}\label{violence-in-video-games}}

There exists a contentious battle between researchers in the field of
media whether or not violent media -- specifically video games --
contributes to violent acts by individuals.

A meta-analysis (Anderson \& Bushman, 2001) of violent videogames
concluded that violent videogames increase aggression, physiological
arousal, and aggression-releated thoughts and feelings -- with small but
positive lationships. Yet, years later, another meta-analysis (Ferguson,
2007) found that videogames were not linked to aggression after
controlling for publication bias. This second meta-analysis also
reported on improvement on visuospatial cognition. Here, too, there was
an issue with publication bias -- but even when adjusting for this, the
improvement effect was still significant.

Now, with conflicting meta-analyses, researchers may be stuck
\emph{inter canem et lupum} when reasoning out how to interpret these
findings. There also exists another bias from either side: the first
publication was presented by Brad Bushman and Craig Anderson -- two
reserachers known for their findings on the influence of violent media
on violent behaviors and attitudes. The second was published by
Christopher John Ferguson, a researcher who quite frequently challenges
these views.

\hypertarget{concluding-remarks}{%
\section{Concluding remarks}\label{concluding-remarks}}

In oppositional contrast to the equating of social sciences as
\enquote{soft sciences} Sanjay Srivastava at the University of Oregon
likes to refer to Psychology as the \enquote{hardest science} has put
together a semi-satirical class syllabus on Psychology research methods
titled \enquote{Everything is F****ed} (Srivastava, n.d.). Here
Dr.~Srivastava has curated a selection of articles to highlight
shortcomings in psychological research that range from uninterpretable
results, failures to replicate, and poor statistical analyses. Yet he
still offers hope the everything is okay. Direct replication is an issue
in psychology - especially in the social realm, but that that does not
mean that all past work is questionable. As mentioned earlier,
\emph{indirect} and \emph{conceptual} replications occur often. For now,
these will have to do to demonstrate that phenomonon of past research is
sound and replicable.

We should leave with a few notes of caution and tips. Research
reproduced outside the original laboratory or collaborators aids more in
demonstrating reproducibiliy than continuous articles from the same
institution. Failure to replicate does not mean that the original effect
is invalid. As the COS has suggested, the issue may even be with the
attempt the replicate and methodological differences that were
overlooked or unnoticed. There may exists some concepts and theories
that are downright wrong and invalid but shotty research continues to
push these to publishers. In the other sense, a good idea may have gone
unnoticed because the first experiment was not successful. But we ought
not to disqualify any large portion of psychology simply because we have
some concerns about reproducibility. Early findings have been a little
grim, but these have tended towards testing recent research rather than
anything foundational.

Not matter the results, reproducibility, openness, and transparancy will
continue to grow from these expeditions. Already the ability to directly
share research is made easy with online storage and sharing. Journals
are requiring data in order to publish -- some even with requirements
that data must be publically available (``Academic journals data sharing
requirements,'' n.d.). Often it may seem, when reading the
interpreations of researcher's work, that hypotheses are strongly
confirmed when the data suggest a weaker magnitude with more exceptions.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-jhl2018academic}{}%
Academic journals data sharing requirements. (n.d.). Johns Hopkins
Libraries, urldate=2018-11-24. Retrieved from
\url{http://dms.data.jhu.edu/data-management-resources/plan-research/funders-data-sharing-requirement/academic-journals-and-data-sharing-requirements/}

\leavevmode\hypertarget{ref-anderson2001effects}{}%
Anderson, C. A., \& Bushman, B. J. (2001). Effects of violent video
games on aggressive behavior, aggressive cognition, aggressive affect,
physiological arousal, and prosocial behavior: A meta-analytic review of
the scientific literature. \emph{Psychological Science}, \emph{12}(5),
353--359.

\leavevmode\hypertarget{ref-braude2002esp}{}%
Braude, S. E. (2002). \emph{ESP and psychokinesis: A philosophical
examination}. Universal-Publishers.

\leavevmode\hypertarget{ref-cohen1962statistical}{}%
Cohen, J. (1962). The statistical power of abnormal-social psychological
research: A review. \emph{The Journal of Abnormal and Social
Psychology}, \emph{65}(3), 145.

\leavevmode\hypertarget{ref-ferguson2007good}{}%
Ferguson, C. J. (2007). The good, the bad and the ugly: A meta-analytic
review of positive and negative effects of violent video games.
\emph{Psychiatric Quarterly}, \emph{78}(4), 309--316.

\leavevmode\hypertarget{ref-ioannidis2005why}{}%
Ioannidis, J. P. (2005). Why most published research findings are false.
\emph{PLoS Medicine}, \emph{2}(8), e124.

\leavevmode\hypertarget{ref-makel2012replications}{}%
Makel, M. C., Plucker, J. A., \& Hegarty, B. (2012). Replications in
psychology research: How often do they really occur? \emph{Perspectives
on Psychological Science}, \emph{7}(6), 537--542.

\leavevmode\hypertarget{ref-meehl1990summaries}{}%
Meehl, P. E. (1990). Why summaries of research on psychological theories
are often uninterpretable. \emph{Psychological Reports}, \emph{66}(1),
195--244.

\leavevmode\hypertarget{ref-open2012open}{}%
Open Science Collaboration. (2012). An open, large-scale, collaborative
effort to estimate the reproducibility of psychological science.
\emph{Perspectives on Psychological Science}, \emph{7}(6), 657--660.
\url{https://doi.org/10.1177/1745691612462588}

\leavevmode\hypertarget{ref-open2015estimating}{}%
Open Science Collaboration. (2015). Estimating the reproducibility of
psychological science. \emph{Science}, \emph{349}(6251).
\url{https://doi.org/10.1126/science.aac4716}

\leavevmode\hypertarget{ref-pashler2012replicability}{}%
Pashler, H., \& Harris, C. R. (2012). Is the replicability crisis
overblown? Three arguments examined. \emph{Perspectives on Psychological
Science}, \emph{7}(6), 531--536.

\leavevmode\hypertarget{ref-shapin1984pump}{}%
Shapin, S. (1984). Pump and circumstance: Robert boyle's literary
technology. \emph{Social Studies of Science}, \emph{14}(4), 481--520.

\leavevmode\hypertarget{ref-srivastava2009making}{}%
Srivastava, S. (2009). Making progress in the hardest sciecne. Retrieved
from
\url{https://thehardestscience.com/2009/03/14/making-progress-in-the-hardest-science/}

\leavevmode\hypertarget{ref-srivastava2018everything}{}%
Srivastava, S. (n.d.). Everything is fucked: The syllabus. Retrieved
from
\url{https://thehardestscience.com/2016/08/11/everything-is-fucked-the-syllabus/}

\leavevmode\hypertarget{ref-sterne2001sifting}{}%
Sterne, J. A., \& Smith, G. D. (2001). Sifting the evidence---what's
wrong with significance tests? \emph{Physical Therapy}, \emph{81}(8),
1464--1469.

\endgroup


\end{document}
